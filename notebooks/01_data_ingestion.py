# flight-delay-prediction-databricks/README.md

# ğŸš€ PrevisÃ£o de Atrasos em Voos com Databricks

Este projeto demonstra um pipeline completo de Machine Learning utilizando a plataforma **Databricks**.
Aqui vocÃª encontrarÃ¡ etapas que abrangem desde a ingestÃ£o de dados com **PySpark**, tratamento com **Delta Lake**, modelagem com **MLflow** e deploy do modelo.

## ğŸ“Š Problema
Prever se um voo sofrerÃ¡ atraso com base em dados histÃ³ricos, como companhia aÃ©rea, horÃ¡rio, origem, destino, etc.

## ğŸ› ï¸ Stack Utilizada
- **Databricks Notebooks**
- **PySpark / Delta Lake**
- **MLflow**
- **scikit-learn**
- **Python 3.9+**

## ğŸ“ Estrutura do Projeto
```
flight-delay-prediction-databricks/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_ingestion.py              # IngestÃ£o de dados com PySpark
â”‚   â”œâ”€â”€ 02_data_cleaning_and_eda.py      # Limpeza, anÃ¡lise exploratÃ³ria
â”‚   â”œâ”€â”€ 03_feature_engineering.py        # CriaÃ§Ã£o e seleÃ§Ã£o de features
â”‚   â”œâ”€â”€ 04_model_training_mlflow.py      # Treinamento e tracking de modelos com MLflow
â”‚   â””â”€â”€ 05_model_evaluation_and_serving.py # AvaliaÃ§Ã£o e exportaÃ§Ã£o para produÃ§Ã£o
â”‚
â”œâ”€â”€ data/                                # InstruÃ§Ãµes ou scripts para ingestÃ£o de dados
â”‚   â””â”€â”€ README_data.md
â”‚
â”œâ”€â”€ requirements.txt                     # DependÃªncias (caso queira testar localmente)
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md                            # Este arquivo
```

## ğŸ” PrÃ©via do Fluxo
1. Leitura de dados CSV reais (ex: [US DOT On-Time](https://www.transtats.bts.gov/OT_Delay/OT_DelayCause1.asp))
2. ConversÃ£o para Delta Lake com versionamento
3. Engenharia de features com PySpark
4. Modelos: Random Forest, Gradient Boosting (usando scikit-learn)
5. Rastreio de experimentos com MLflow
6. AvaliaÃ§Ã£o com mÃ©tricas padrÃ£o
7. (Opcional) Deploy com MLflow Model Registry ou Serving

## ğŸ“¸ Screenshots (coloque imagens do seu ambiente Databricks aqui)
- Notebooks
- MLflow Runs
- VisualizaÃ§Ãµes EDA

## ğŸ“Œ Como Executar
1. Importe o projeto como **repo Git** no seu workspace Databricks
2. Siga os notebooks na ordem sequencial
3. Customize seu pipeline conforme desejado

## âœï¸ Autor
Gabriel Kaahara â€“ [github.com/gabrielkaahara](https://github.com/gabrielkaahara)

---

# notebooks/01_data_ingestion.py

# Databricks notebook source
# MAGIC %md
# MAGIC ## IngestÃ£o de Dados: Atrasos em Voos
# MAGIC Utilizando PySpark + Delta Lake

# COMMAND ----------
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Substitua este caminho pelo caminho do seu dataset (ou use Autoloader se estiver na cloud)
df = spark.read.option("header", True).csv("/dbfs/FileStore/data/voos.csv")
df.printSchema()
df.display()

# COMMAND ----------
# Salvar em Delta Lake
(df.write
 .format("delta")
 .mode("overwrite")
 .save("/delta/voos"))

# CriaÃ§Ã£o da tabela Delta (opcional)
spark.sql("CREATE TABLE IF NOT EXISTS voos_delta USING DELTA LOCATION '/delta/voos'")

# COMMAND ----------
# VisualizaÃ§Ã£o bÃ¡sica
df.groupBy("CARRIER").count().orderBy("count", ascending=False).show()

# COMMAND ----------
# Continue com limpeza e EDA no prÃ³ximo notebook...
